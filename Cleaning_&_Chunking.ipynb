{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcPeKdIT0CjG",
        "outputId": "dd9155bd-5eb0-4b39-86f7-b36df70f8d23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.11/dist-packages (0.3.9)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain-text-splitters) (0.3.72)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (2.11.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-text-splitters) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF tiktoken langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz # PyMuPDF\n",
        "import tiktoken\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "9T6cbuTG1c1R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import RecursiveCharacterTextSplitter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- Configuration ---\n",
        "# The target chunk size in tokens. A common value is 512.\n",
        "CHUNK_SIZE_TOKENS = 512\n",
        "# The overlap percentage between consecutive chunks (e.g., 0.15 for 15% overlap).\n",
        "OVERLAP_PERCENTAGE = 0.15\n",
        "# Encoding for tokenization (e.g., 'cl100k_base' for OpenAI models like GPT-4, GPT-3.5)\n",
        "ENCODING_NAME = \"cl100k_base\"\n",
        "\n",
        "# Heuristic for identifying common headers/footers\n",
        "# Max number of lines from top/bottom of a page to consider as potential header/footer\n",
        "MAX_LINES_TO_CHECK = 5\n",
        "# Percentage of pages a line must appear on (excluding page 1) to be considered a common header/footer\n",
        "REPETITION_THRESHOLD_PERCENT = 70\n",
        "\n",
        "# Initialize tiktoken encoder globally for consistent token counting\n",
        "ENCODER = tiktoken.get_encoding(ENCODING_NAME)\n"
      ],
      "metadata": {
        "id": "957alisx11HE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(text: str) -> int:\n",
        "    \"\"\"Counts tokens using the global tiktoken encoder.\"\"\"\n",
        "    return len(ENCODER.encode(text))\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Extracts text content page by page from a PDF document.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The file path to the PDF document.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of dictionaries, each containing 'page_num' and 'text'\n",
        "                    for a page. Returns an empty list if the file cannot be opened.\n",
        "    \"\"\"\n",
        "    pages_content = []\n",
        "    try:\n",
        "        document = fitz.open(pdf_path)\n",
        "        for page_num in range(len(document)):\n",
        "            page = document.load_page(page_num)\n",
        "            text = page.get_text(\"text\")\n",
        "            pages_content.append({\"page_num\": page_num + 1, \"text\": text})\n",
        "        document.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF {pdf_path}: {e}\")\n",
        "    return pages_content"
      ],
      "metadata": {
        "id": "BeLvqRkw35DG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_common_page_elements(all_pages_content: dict[str, list[dict]],\n",
        "                                   max_lines: int = MAX_LINES_TO_CHECK,\n",
        "                                   repetition_threshold_percent: int = REPETITION_THRESHOLD_PERCENT) -> tuple[set, set]:\n",
        "    \"\"\"\n",
        "    Analyzes text from multiple pages (excluding first pages) to identify common\n",
        "    header and footer lines based on repetition.\n",
        "\n",
        "    Args:\n",
        "        all_pages_content (dict[str, list[dict]]): Dictionary where keys are doc_ids\n",
        "                                                    and values are lists of page_data.\n",
        "        max_lines (int): Max number of lines from top/bottom to consider.\n",
        "        repetition_threshold_percent (int): Percentage of non-first pages a line must\n",
        "                                            appear on to be considered common.\n",
        "\n",
        "    Returns:\n",
        "        tuple[set, set]: Two sets: (common_header_lines, common_footer_lines).\n",
        "    \"\"\"\n",
        "    header_candidates = Counter()\n",
        "    footer_candidates = Counter()\n",
        "    total_non_first_pages = 0\n",
        "\n",
        "    for doc_id, pages_data in all_pages_content.items():\n",
        "        for page_data in pages_data:\n",
        "            page_num = page_data['page_num']\n",
        "            page_text = page_data['text']\n",
        "\n",
        "            # Skip the first page of each document for common element identification\n",
        "            if page_num == 1:\n",
        "                continue\n",
        "\n",
        "            total_non_first_pages += 1\n",
        "            lines = [line.strip() for line in page_text.split('\\n') if line.strip()]\n",
        "\n",
        "            # Collect header candidates\n",
        "            for i in range(min(max_lines, len(lines))):\n",
        "                header_candidates[lines[i]] += 1\n",
        "\n",
        "            # Collect footer candidates (from the end of the page)\n",
        "            for i in range(max(0, len(lines) - max_lines), len(lines)):\n",
        "                footer_candidates[lines[i]] += 1\n",
        "\n",
        "    common_header_lines = set()\n",
        "    common_footer_lines = set()\n",
        "\n",
        "    if total_non_first_pages == 0:\n",
        "        print(\"No non-first pages found to identify common elements.\")\n",
        "        return common_header_lines, common_footer_lines\n",
        "\n",
        "    threshold_count = math.ceil(total_non_first_pages * (repetition_threshold_percent / 100))\n",
        "    print(f\"Identifying common elements: Total non-first pages: {total_non_first_pages}, Threshold count: {threshold_count}\")\n",
        "\n",
        "    for line, count in header_candidates.items():\n",
        "        if count >= threshold_count:\n",
        "            common_header_lines.add(line)\n",
        "            print(f\"  Identified common header: '{line}' (appears {count} times)\")\n",
        "\n",
        "    for line, count in footer_candidates.items():\n",
        "        # A common heuristic for page numbers: remove if it's just a number or \"Page X\"\n",
        "        if count >= threshold_count and (re.fullmatch(r'\\s*\\d+\\s*', line) or re.fullmatch(r'Page\\s+\\d+\\s*(of\\s+\\d+)?', line, re.IGNORECASE)):\n",
        "            common_footer_lines.add(line)\n",
        "            print(f\"  Identified common footer: '{line}' (appears {count} times)\")\n",
        "        # You can extend this logic to include other non-numeric common footers if needed\n",
        "\n",
        "    return common_header_lines, common_footer_lines"
      ],
      "metadata": {
        "id": "wb-iU3qjHE0v"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_identified_elements(page_text: str, page_num: int,\n",
        "                               common_header_lines: set, common_footer_lines: set) -> str:\n",
        "    \"\"\"\n",
        "    Removes identified common header and footer lines from a page's text.\n",
        "    It skips removal for the first page.\n",
        "\n",
        "    Args:\n",
        "        page_text (str): The text content of a single page.\n",
        "        page_num (int): The current page number (1-indexed).\n",
        "        common_header_lines (set): Set of lines identified as common headers.\n",
        "        common_footer_lines (set): Set of lines identified as common footers.\n",
        "\n",
        "    Returns:\n",
        "        str: The page text with identified common elements removed.\n",
        "    \"\"\"\n",
        "    # Skip removal for the first page, as it contains unique, important metadata.\n",
        "    if page_num == 1:\n",
        "        return page_text\n",
        "\n",
        "    lines = [line.strip() for line in page_text.split('\\n')]\n",
        "    cleaned_lines = []\n",
        "\n",
        "    # Flags to stop removal once non-header/non-footer content is found\n",
        "    header_removal_done = False\n",
        "    footer_removal_done = False\n",
        "\n",
        "    # Process lines from top for header removal\n",
        "    for i, line in enumerate(lines):\n",
        "        if not header_removal_done and line in common_header_lines:\n",
        "            # This line is a common header, skip it\n",
        "            continue\n",
        "        else:\n",
        "            # Found non-header content or no more headers, stop checking\n",
        "            header_removal_done = True\n",
        "            cleaned_lines.append(line) # Add this line and subsequent lines\n",
        "\n",
        "    # Now process the cleaned lines from the bottom for footer removal\n",
        "    # This is a bit tricky with `cleaned_lines` already built.\n",
        "    # A simpler approach for this heuristic is to rebuild `cleaned_lines` from scratch\n",
        "    # by iterating through original lines and marking for inclusion/exclusion.\n",
        "\n",
        "    # Re-process original lines for both header and footer removal in one pass\n",
        "    final_lines = []\n",
        "\n",
        "    # Determine which lines to keep from the top (non-headers)\n",
        "    temp_lines = []\n",
        "    for i, line in enumerate(lines):\n",
        "        if line in common_header_lines and i < MAX_LINES_TO_CHECK: # Only consider top lines for header\n",
        "            continue # Skip this header line\n",
        "        else:\n",
        "            temp_lines.append(line)\n",
        "\n",
        "    # Determine which lines to keep from the bottom (non-footers)\n",
        "    # Iterate from the end of temp_lines\n",
        "    footer_check_start_index = max(0, len(temp_lines) - MAX_LINES_TO_CHECK)\n",
        "    for i, line in enumerate(temp_lines):\n",
        "        if line in common_footer_lines and i >= footer_check_start_index: # Only consider bottom lines for footer\n",
        "            continue # Skip this footer line\n",
        "        else:\n",
        "            final_lines.append(line)\n",
        "\n",
        "    # Filter out any completely empty lines that result from removal\n",
        "    return \"\\n\".join(line for line in final_lines if line.strip() != \"\")"
      ],
      "metadata": {
        "id": "VFjdLXJtBxTI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text_with_metadata(text: str, chunk_size_tokens: int, overlap_percentage: float,\n",
        "                             doc_id: str, page: int, base_clause_id_prefix: str):\n",
        "    \"\"\"\n",
        "    Splits a given text into chunks using RecursiveCharacterTextSplitter,\n",
        "    prioritizing natural language boundaries. Each chunk is tagged with metadata.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text content to be chunked.\n",
        "        chunk_size_tokens (int): The target maximum number of tokens per chunk.\n",
        "                                 This is converted to character length for the splitter.\n",
        "        overlap_percentage (float): The percentage of overlap between consecutive chunks.\n",
        "        doc_id (str): The ID of the document (e.g., \"policy_123\").\n",
        "        page (int): The page number the content is notionally from.\n",
        "        base_clause_id_prefix (str): A prefix for generating clause IDs (e.g., \"1.1\").\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of dictionaries, where each dictionary represents a chunk\n",
        "                    and contains its content and metadata.\n",
        "    \"\"\"\n",
        "    # Estimate character length based on average tokens per character\n",
        "    avg_chars_per_token = 4 # Common average for English text\n",
        "    chunk_size_chars = chunk_size_tokens * avg_chars_per_token\n",
        "    overlap_chars = math.floor(chunk_size_chars * overlap_percentage)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size_chars,\n",
        "        chunk_overlap=overlap_chars,\n",
        "        length_function=len, # Use character length for splitting\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Prioritize paragraphs, then lines, then words, then characters\n",
        "    )\n",
        "\n",
        "    raw_chunks = text_splitter.split_text(text)\n",
        "\n",
        "    processed_chunks = []\n",
        "    for i, chunk_content in enumerate(raw_chunks):\n",
        "        token_length = count_tokens(chunk_content)\n",
        "        clause_id = f\"{base_clause_id_prefix}-{doc_id}-p{page}-c{i + 1}\"\n",
        "\n",
        "        metadata = {\n",
        "            \"doc_id\": doc_id,\n",
        "            \"page\": page,\n",
        "            \"clause_id\": clause_id,\n",
        "            \"chunk_length_tokens\": token_length,\n",
        "            \"chunk_length_chars\": len(chunk_content)\n",
        "        }\n",
        "\n",
        "        processed_chunks.append({\n",
        "            \"content\": chunk_content,\n",
        "            \"metadata\": metadata\n",
        "        })\n",
        "\n",
        "    return processed_chunks"
      ],
      "metadata": {
        "id": "BNBtJqqi4HGq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdfs_for_chunking(pdf_paths: list[str]):\n",
        "    \"\"\"\n",
        "    Processes a list of PDF file paths, extracts text, dynamically identifies\n",
        "    and removes common headers/footers, and chunks the cleaned text.\n",
        "\n",
        "    Args:\n",
        "        pdf_paths (list[str]): A list of file paths to the PDF documents.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A flattened list of all processed chunks from all PDFs.\n",
        "    \"\"\"\n",
        "    all_docs_pages_content = {}\n",
        "    # First pass: Extract all text and store it for common element identification\n",
        "    for pdf_path in pdf_paths:\n",
        "        doc_id = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "        pages_content = extract_text_from_pdf(pdf_path)\n",
        "        if pages_content:\n",
        "            all_docs_pages_content[doc_id] = pages_content\n",
        "        else:\n",
        "            print(f\"Skipping {pdf_path} due to extraction errors.\")\n",
        "\n",
        "    # Identify common headers and footers across all documents (excluding first pages)\n",
        "    common_header_lines, common_footer_lines = identify_common_page_elements(all_docs_pages_content)\n",
        "\n",
        "    all_processed_chunks = []\n",
        "    # Second pass: Process each page, remove identified common elements, and chunk\n",
        "    for doc_id, pages_data in all_docs_pages_content.items():\n",
        "        print(f\"\\n--- Chunking Document: {doc_id} ---\")\n",
        "        for page_data in pages_data:\n",
        "            page_num = page_data['page_num']\n",
        "            raw_page_text = page_data['text']\n",
        "\n",
        "            # --- Dynamically remove identified common headers/footers ---\n",
        "            cleaned_page_text = remove_identified_elements(\n",
        "                raw_page_text, page_num, common_header_lines, common_footer_lines\n",
        "            )\n",
        "\n",
        "            print(f\"  Processing Page {page_num} (raw length: {len(raw_page_text)} chars, cleaned length: {len(cleaned_page_text)} chars)\")\n",
        "\n",
        "            if not cleaned_page_text.strip():\n",
        "                print(f\"    Page {page_num} became empty after cleaning. Skipping chunking for this page.\")\n",
        "                continue\n",
        "\n",
        "            page_chunks = chunk_text_with_metadata(\n",
        "                text=cleaned_page_text,\n",
        "                chunk_size_tokens=CHUNK_SIZE_TOKENS,\n",
        "                overlap_percentage=OVERLAP_PERCENTAGE,\n",
        "                doc_id=doc_id,\n",
        "                page=page_num,\n",
        "                base_clause_id_prefix=\"Clause\"\n",
        "            )\n",
        "            all_processed_chunks.extend(page_chunks)\n",
        "            print(f\"    Generated {len(page_chunks)} chunks for page {page_num}.\")\n",
        "\n",
        "    return all_processed_chunks"
      ],
      "metadata": {
        "id": "-RyO_NJH4Xqv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example Usage with Placeholder PDF Paths ---\n",
        "# IMPORTANT: Replace these with the actual paths to your uploaded PDF files in Colab\n",
        "pdf_file_paths = [\n",
        "    \"/content/BAJHLIP23020V012223.pdf\",\n",
        "    \"/content/CHOTGDP23004V012223.pdf\",\n",
        "    \"/content/EDLHLGA23009V012223.pdf\",\n",
        "    \"/content/HDFHLIP23024V072223.pdf\",\n",
        "    \"/content/ICIHLIP22012V012223.pdf\"\n",
        "]\n",
        "\n",
        "# Check if placeholder files exist (they won't unless you upload them)\n",
        "# This is just for demonstration; in a real scenario, you'd ensure files are there.\n",
        "existing_pdf_paths = [p for p in pdf_file_paths if os.path.exists(p)]\n"
      ],
      "metadata": {
        "id": "fwbk9W6r6BbK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not existing_pdf_paths:\n",
        "    print(\"WARNING: No PDF files found at the specified paths. Please upload your PDFs to Colab\")\n",
        "    print(\"and update the 'pdf_file_paths' list with their correct locations.\")\n",
        "    print(\"Proceeding with a dummy text for demonstration purposes as no PDFs were found.\")\n",
        "    dummy_text_page1 = \"\"\"\n",
        "    Policy Title: Comprehensive Health Plan 2025\n",
        "    Version 1.0 - Effective Date: Jan 1, 2025\n",
        "\n",
        "    This is the unique content for page 1.\n",
        "    \"\"\"\n",
        "    dummy_text_page2 = \"\"\"\n",
        "    Common Header Text\n",
        "    Section 1: Eligibility\n",
        "\n",
        "    1.1 Age Requirements:\n",
        "    Applicants must be between 18 and 65 years old. Dependents up to 26 are covered if full-time students.\n",
        "\n",
        "    Common Footer Text - Page 2\n",
        "    \"\"\"\n",
        "    dummy_text_page3 = \"\"\"\n",
        "    Common Header Text\n",
        "    1.2 Geographic Coverage:\n",
        "    Coverage is valid in all 50 US states. Travel abroad is limited to 90 days.\n",
        "\n",
        "    Table 1: Deductibles\n",
        "    | Plan Type | Deductible | Co-pay |\n",
        "    |---|---|---|\n",
        "    | Basic | $1000 | $50 |\n",
        "    | Premium | $500 | $25 |\n",
        "\n",
        "    Common Footer Text - Page 3\n",
        "    \"\"\"\n",
        "\n",
        "    # Simulate multiple pages for a single dummy document to test common element identification\n",
        "    simulated_pages_content = {\n",
        "        \"dummy_doc\": [\n",
        "            {\"page_num\": 1, \"text\": dummy_text_page1},\n",
        "            {\"page_num\": 2, \"text\": dummy_text_page2},\n",
        "            {\"page_num\": 3, \"text\": dummy_text_page3},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Manually call the two-pass process for the dummy data\n",
        "    print(\"\\n--- Generating chunks from dummy text for demonstration ---\")\n",
        "    common_header_lines_dummy, common_footer_lines_dummy = identify_common_page_elements(simulated_pages_content)\n",
        "\n",
        "    processed_chunks = []\n",
        "    for doc_id, pages_data in simulated_pages_content.items():\n",
        "        for page_data in pages_data:\n",
        "            cleaned_page_text = remove_identified_elements(\n",
        "                page_data['text'], page_data['page_num'], common_header_lines_dummy, common_footer_lines_dummy\n",
        "            )\n",
        "            if cleaned_page_text.strip():\n",
        "                page_chunks = chunk_text_with_metadata(\n",
        "                    text=cleaned_page_text,\n",
        "                    chunk_size_tokens=CHUNK_SIZE_TOKENS,\n",
        "                    overlap_percentage=OVERLAP_PERCENTAGE,\n",
        "                    doc_id=doc_id,\n",
        "                    page=page_data['page_num'],\n",
        "                    base_clause_id_prefix=\"DummyClause\"\n",
        "                )\n",
        "                processed_chunks.extend(page_chunks)\n",
        "\n",
        "else:\n",
        "    print(f\"--- Starting PDF Processing and Chunking for {len(existing_pdf_paths)} files ---\")\n",
        "    processed_chunks = process_pdfs_for_chunking(existing_pdf_paths)\n",
        "    print(f\"\\n--- Total Generated Chunks: {len(processed_chunks)} ---\")\n",
        "\n",
        "# Display a few sample chunks\n",
        "if processed_chunks:\n",
        "    print(\"\\n--- Sample of Processed Chunks (first 5) ---\")\n",
        "    for i, chunk in enumerate(processed_chunks[:5]):\n",
        "        print(f\"\\nChunk {i+1}:\")\n",
        "        print(f\"  Metadata: {json.dumps(chunk['metadata'], indent=2)}\")\n",
        "        print(f\"  Content (first 200 chars): \\\"{chunk['content'][:200]}...\\\"\")\n",
        "        print(f\"  Content length (chars): {len(chunk['content'])}\")\n",
        "        print(f\"  Content length (tokens): {chunk['metadata']['chunk_length_tokens']}\")\n",
        "else:\n",
        "    print(\"No chunks were generated. Please ensure your PDFs are uploaded and paths are correct.\")\n",
        "\n",
        "print(f\"\\n--- Document Processing & Chunking Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNrVAn4J6vIj",
        "outputId": "d66fda0d-6d4d-4d21-aa88-12f7a406defa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting PDF Processing and Chunking for 5 files ---\n",
            "Identifying common elements: Total non-first pages: 217, Threshold count: 152\n",
            "\n",
            "--- Chunking Document: BAJHLIP23020V012223 ---\n",
            "  Processing Page 1 (raw length: 4505 chars, cleaned length: 4505 chars)\n",
            "    Generated 3 chunks for page 1.\n",
            "  Processing Page 2 (raw length: 3985 chars, cleaned length: 3815 chars)\n",
            "    Generated 3 chunks for page 2.\n",
            "  Processing Page 3 (raw length: 4461 chars, cleaned length: 4303 chars)\n",
            "    Generated 3 chunks for page 3.\n",
            "  Processing Page 4 (raw length: 3890 chars, cleaned length: 3721 chars)\n",
            "    Generated 2 chunks for page 4.\n",
            "  Processing Page 5 (raw length: 4371 chars, cleaned length: 4206 chars)\n",
            "    Generated 3 chunks for page 5.\n",
            "  Processing Page 6 (raw length: 4644 chars, cleaned length: 4484 chars)\n",
            "    Generated 3 chunks for page 6.\n",
            "  Processing Page 7 (raw length: 4517 chars, cleaned length: 4353 chars)\n",
            "    Generated 3 chunks for page 7.\n",
            "  Processing Page 8 (raw length: 3900 chars, cleaned length: 3733 chars)\n",
            "    Generated 2 chunks for page 8.\n",
            "  Processing Page 9 (raw length: 3982 chars, cleaned length: 3814 chars)\n",
            "    Generated 3 chunks for page 9.\n",
            "  Processing Page 10 (raw length: 3762 chars, cleaned length: 3596 chars)\n",
            "    Generated 2 chunks for page 10.\n",
            "  Processing Page 11 (raw length: 4319 chars, cleaned length: 4158 chars)\n",
            "    Generated 3 chunks for page 11.\n",
            "  Processing Page 12 (raw length: 3812 chars, cleaned length: 3658 chars)\n",
            "    Generated 2 chunks for page 12.\n",
            "  Processing Page 13 (raw length: 3136 chars, cleaned length: 2927 chars)\n",
            "    Generated 2 chunks for page 13.\n",
            "  Processing Page 14 (raw length: 4430 chars, cleaned length: 4266 chars)\n",
            "    Generated 3 chunks for page 14.\n",
            "  Processing Page 15 (raw length: 4624 chars, cleaned length: 4463 chars)\n",
            "    Generated 3 chunks for page 15.\n",
            "  Processing Page 16 (raw length: 4805 chars, cleaned length: 4644 chars)\n",
            "    Generated 3 chunks for page 16.\n",
            "  Processing Page 17 (raw length: 4591 chars, cleaned length: 4427 chars)\n",
            "    Generated 3 chunks for page 17.\n",
            "  Processing Page 18 (raw length: 4348 chars, cleaned length: 4189 chars)\n",
            "    Generated 3 chunks for page 18.\n",
            "  Processing Page 19 (raw length: 2861 chars, cleaned length: 2663 chars)\n",
            "    Generated 2 chunks for page 19.\n",
            "  Processing Page 20 (raw length: 3654 chars, cleaned length: 3469 chars)\n",
            "    Generated 2 chunks for page 20.\n",
            "  Processing Page 21 (raw length: 3662 chars, cleaned length: 3491 chars)\n",
            "    Generated 2 chunks for page 21.\n",
            "  Processing Page 22 (raw length: 3914 chars, cleaned length: 3749 chars)\n",
            "    Generated 2 chunks for page 22.\n",
            "  Processing Page 23 (raw length: 5505 chars, cleaned length: 5352 chars)\n",
            "    Generated 3 chunks for page 23.\n",
            "  Processing Page 24 (raw length: 4141 chars, cleaned length: 3974 chars)\n",
            "    Generated 3 chunks for page 24.\n",
            "  Processing Page 25 (raw length: 3892 chars, cleaned length: 3721 chars)\n",
            "    Generated 2 chunks for page 25.\n",
            "  Processing Page 26 (raw length: 4076 chars, cleaned length: 3914 chars)\n",
            "    Generated 3 chunks for page 26.\n",
            "  Processing Page 27 (raw length: 5446 chars, cleaned length: 5289 chars)\n",
            "    Generated 3 chunks for page 27.\n",
            "  Processing Page 28 (raw length: 4011 chars, cleaned length: 3857 chars)\n",
            "    Generated 2 chunks for page 28.\n",
            "  Processing Page 29 (raw length: 4458 chars, cleaned length: 4294 chars)\n",
            "    Generated 3 chunks for page 29.\n",
            "  Processing Page 30 (raw length: 4009 chars, cleaned length: 3832 chars)\n",
            "    Generated 2 chunks for page 30.\n",
            "  Processing Page 31 (raw length: 4987 chars, cleaned length: 4823 chars)\n",
            "    Generated 3 chunks for page 31.\n",
            "  Processing Page 32 (raw length: 4295 chars, cleaned length: 4115 chars)\n",
            "    Generated 3 chunks for page 32.\n",
            "  Processing Page 33 (raw length: 4415 chars, cleaned length: 4244 chars)\n",
            "    Generated 3 chunks for page 33.\n",
            "  Processing Page 34 (raw length: 4780 chars, cleaned length: 4609 chars)\n",
            "    Generated 3 chunks for page 34.\n",
            "  Processing Page 35 (raw length: 5106 chars, cleaned length: 4942 chars)\n",
            "    Generated 3 chunks for page 35.\n",
            "  Processing Page 36 (raw length: 4883 chars, cleaned length: 4726 chars)\n",
            "    Generated 3 chunks for page 36.\n",
            "  Processing Page 37 (raw length: 4405 chars, cleaned length: 4230 chars)\n",
            "    Generated 3 chunks for page 37.\n",
            "  Processing Page 38 (raw length: 4856 chars, cleaned length: 4691 chars)\n",
            "    Generated 3 chunks for page 38.\n",
            "  Processing Page 39 (raw length: 4833 chars, cleaned length: 4672 chars)\n",
            "    Generated 3 chunks for page 39.\n",
            "  Processing Page 40 (raw length: 4069 chars, cleaned length: 3900 chars)\n",
            "    Generated 3 chunks for page 40.\n",
            "  Processing Page 41 (raw length: 2512 chars, cleaned length: 2349 chars)\n",
            "    Generated 2 chunks for page 41.\n",
            "  Processing Page 42 (raw length: 3086 chars, cleaned length: 2915 chars)\n",
            "    Generated 2 chunks for page 42.\n",
            "  Processing Page 43 (raw length: 2881 chars, cleaned length: 2699 chars)\n",
            "    Generated 2 chunks for page 43.\n",
            "  Processing Page 44 (raw length: 3302 chars, cleaned length: 3118 chars)\n",
            "    Generated 2 chunks for page 44.\n",
            "  Processing Page 45 (raw length: 3419 chars, cleaned length: 3235 chars)\n",
            "    Generated 2 chunks for page 45.\n",
            "  Processing Page 46 (raw length: 3363 chars, cleaned length: 3182 chars)\n",
            "    Generated 2 chunks for page 46.\n",
            "  Processing Page 47 (raw length: 3459 chars, cleaned length: 3272 chars)\n",
            "    Generated 2 chunks for page 47.\n",
            "  Processing Page 48 (raw length: 2250 chars, cleaned length: 2064 chars)\n",
            "    Generated 2 chunks for page 48.\n",
            "  Processing Page 49 (raw length: 2332 chars, cleaned length: 2151 chars)\n",
            "    Generated 2 chunks for page 49.\n",
            "\n",
            "--- Chunking Document: CHOTGDP23004V012223 ---\n",
            "  Processing Page 1 (raw length: 668 chars, cleaned length: 668 chars)\n",
            "    Generated 1 chunks for page 1.\n",
            "  Processing Page 2 (raw length: 4006 chars, cleaned length: 3754 chars)\n",
            "    Generated 3 chunks for page 2.\n",
            "  Processing Page 3 (raw length: 4434 chars, cleaned length: 4179 chars)\n",
            "    Generated 3 chunks for page 3.\n",
            "  Processing Page 4 (raw length: 4418 chars, cleaned length: 4165 chars)\n",
            "    Generated 3 chunks for page 4.\n",
            "  Processing Page 5 (raw length: 5063 chars, cleaned length: 4814 chars)\n",
            "    Generated 3 chunks for page 5.\n",
            "  Processing Page 6 (raw length: 4815 chars, cleaned length: 4566 chars)\n",
            "    Generated 3 chunks for page 6.\n",
            "  Processing Page 7 (raw length: 4948 chars, cleaned length: 4700 chars)\n",
            "    Generated 3 chunks for page 7.\n",
            "  Processing Page 8 (raw length: 4277 chars, cleaned length: 4015 chars)\n",
            "    Generated 3 chunks for page 8.\n",
            "  Processing Page 9 (raw length: 3219 chars, cleaned length: 2959 chars)\n",
            "    Generated 2 chunks for page 9.\n",
            "  Processing Page 10 (raw length: 3902 chars, cleaned length: 3635 chars)\n",
            "    Generated 2 chunks for page 10.\n",
            "  Processing Page 11 (raw length: 3262 chars, cleaned length: 2998 chars)\n",
            "    Generated 2 chunks for page 11.\n",
            "  Processing Page 12 (raw length: 3125 chars, cleaned length: 2854 chars)\n",
            "    Generated 2 chunks for page 12.\n",
            "  Processing Page 13 (raw length: 3972 chars, cleaned length: 3711 chars)\n",
            "    Generated 2 chunks for page 13.\n",
            "  Processing Page 14 (raw length: 4207 chars, cleaned length: 3955 chars)\n",
            "    Generated 3 chunks for page 14.\n",
            "  Processing Page 15 (raw length: 3238 chars, cleaned length: 2973 chars)\n",
            "    Generated 2 chunks for page 15.\n",
            "  Processing Page 16 (raw length: 3952 chars, cleaned length: 3698 chars)\n",
            "    Generated 2 chunks for page 16.\n",
            "  Processing Page 17 (raw length: 3666 chars, cleaned length: 3407 chars)\n",
            "    Generated 2 chunks for page 17.\n",
            "  Processing Page 18 (raw length: 4069 chars, cleaned length: 3806 chars)\n",
            "    Generated 2 chunks for page 18.\n",
            "  Processing Page 19 (raw length: 4039 chars, cleaned length: 3773 chars)\n",
            "    Generated 3 chunks for page 19.\n",
            "  Processing Page 20 (raw length: 4300 chars, cleaned length: 4043 chars)\n",
            "    Generated 3 chunks for page 20.\n",
            "  Processing Page 21 (raw length: 4268 chars, cleaned length: 4014 chars)\n",
            "    Generated 3 chunks for page 21.\n",
            "  Processing Page 22 (raw length: 3679 chars, cleaned length: 3424 chars)\n",
            "    Generated 2 chunks for page 22.\n",
            "  Processing Page 23 (raw length: 3462 chars, cleaned length: 3170 chars)\n",
            "    Generated 2 chunks for page 23.\n",
            "  Processing Page 24 (raw length: 3341 chars, cleaned length: 3066 chars)\n",
            "    Generated 2 chunks for page 24.\n",
            "  Processing Page 25 (raw length: 3003 chars, cleaned length: 2714 chars)\n",
            "    Generated 2 chunks for page 25.\n",
            "  Processing Page 26 (raw length: 2483 chars, cleaned length: 2195 chars)\n",
            "    Generated 2 chunks for page 26.\n",
            "  Processing Page 27 (raw length: 2466 chars, cleaned length: 2181 chars)\n",
            "    Generated 2 chunks for page 27.\n",
            "  Processing Page 28 (raw length: 4001 chars, cleaned length: 3739 chars)\n",
            "    Generated 2 chunks for page 28.\n",
            "  Processing Page 29 (raw length: 3595 chars, cleaned length: 3336 chars)\n",
            "    Generated 2 chunks for page 29.\n",
            "  Processing Page 30 (raw length: 4161 chars, cleaned length: 3911 chars)\n",
            "    Generated 3 chunks for page 30.\n",
            "  Processing Page 31 (raw length: 3356 chars, cleaned length: 3090 chars)\n",
            "    Generated 2 chunks for page 31.\n",
            "  Processing Page 32 (raw length: 4144 chars, cleaned length: 3881 chars)\n",
            "    Generated 3 chunks for page 32.\n",
            "  Processing Page 33 (raw length: 3892 chars, cleaned length: 3631 chars)\n",
            "    Generated 2 chunks for page 33.\n",
            "  Processing Page 34 (raw length: 3090 chars, cleaned length: 2828 chars)\n",
            "    Generated 2 chunks for page 34.\n",
            "  Processing Page 35 (raw length: 4124 chars, cleaned length: 3862 chars)\n",
            "    Generated 3 chunks for page 35.\n",
            "  Processing Page 36 (raw length: 3840 chars, cleaned length: 3575 chars)\n",
            "    Generated 2 chunks for page 36.\n",
            "  Processing Page 37 (raw length: 3774 chars, cleaned length: 3506 chars)\n",
            "    Generated 2 chunks for page 37.\n",
            "  Processing Page 38 (raw length: 3696 chars, cleaned length: 3428 chars)\n",
            "    Generated 2 chunks for page 38.\n",
            "  Processing Page 39 (raw length: 3428 chars, cleaned length: 3165 chars)\n",
            "    Generated 2 chunks for page 39.\n",
            "  Processing Page 40 (raw length: 3221 chars, cleaned length: 2961 chars)\n",
            "    Generated 2 chunks for page 40.\n",
            "  Processing Page 41 (raw length: 3127 chars, cleaned length: 2863 chars)\n",
            "    Generated 2 chunks for page 41.\n",
            "  Processing Page 42 (raw length: 3168 chars, cleaned length: 2907 chars)\n",
            "    Generated 2 chunks for page 42.\n",
            "  Processing Page 43 (raw length: 3437 chars, cleaned length: 3175 chars)\n",
            "    Generated 2 chunks for page 43.\n",
            "  Processing Page 44 (raw length: 3176 chars, cleaned length: 2908 chars)\n",
            "    Generated 2 chunks for page 44.\n",
            "  Processing Page 45 (raw length: 3589 chars, cleaned length: 3333 chars)\n",
            "    Generated 2 chunks for page 45.\n",
            "  Processing Page 46 (raw length: 3668 chars, cleaned length: 3401 chars)\n",
            "    Generated 2 chunks for page 46.\n",
            "  Processing Page 47 (raw length: 3737 chars, cleaned length: 3475 chars)\n",
            "    Generated 2 chunks for page 47.\n",
            "  Processing Page 48 (raw length: 4229 chars, cleaned length: 3970 chars)\n",
            "    Generated 3 chunks for page 48.\n",
            "  Processing Page 49 (raw length: 3464 chars, cleaned length: 3198 chars)\n",
            "    Generated 2 chunks for page 49.\n",
            "  Processing Page 50 (raw length: 4316 chars, cleaned length: 4057 chars)\n",
            "    Generated 3 chunks for page 50.\n",
            "  Processing Page 51 (raw length: 3639 chars, cleaned length: 3379 chars)\n",
            "    Generated 2 chunks for page 51.\n",
            "  Processing Page 52 (raw length: 3809 chars, cleaned length: 3549 chars)\n",
            "    Generated 2 chunks for page 52.\n",
            "  Processing Page 53 (raw length: 3768 chars, cleaned length: 3507 chars)\n",
            "    Generated 2 chunks for page 53.\n",
            "  Processing Page 54 (raw length: 3651 chars, cleaned length: 3400 chars)\n",
            "    Generated 2 chunks for page 54.\n",
            "  Processing Page 55 (raw length: 3931 chars, cleaned length: 3669 chars)\n",
            "    Generated 2 chunks for page 55.\n",
            "  Processing Page 56 (raw length: 3377 chars, cleaned length: 3119 chars)\n",
            "    Generated 2 chunks for page 56.\n",
            "  Processing Page 57 (raw length: 3396 chars, cleaned length: 3137 chars)\n",
            "    Generated 2 chunks for page 57.\n",
            "  Processing Page 58 (raw length: 2529 chars, cleaned length: 2237 chars)\n",
            "    Generated 2 chunks for page 58.\n",
            "  Processing Page 59 (raw length: 3190 chars, cleaned length: 2920 chars)\n",
            "    Generated 2 chunks for page 59.\n",
            "  Processing Page 60 (raw length: 3786 chars, cleaned length: 3524 chars)\n",
            "    Generated 2 chunks for page 60.\n",
            "  Processing Page 61 (raw length: 3961 chars, cleaned length: 3702 chars)\n",
            "    Generated 2 chunks for page 61.\n",
            "  Processing Page 62 (raw length: 3858 chars, cleaned length: 3593 chars)\n",
            "    Generated 2 chunks for page 62.\n",
            "  Processing Page 63 (raw length: 3988 chars, cleaned length: 3723 chars)\n",
            "    Generated 2 chunks for page 63.\n",
            "  Processing Page 64 (raw length: 3218 chars, cleaned length: 2950 chars)\n",
            "    Generated 2 chunks for page 64.\n",
            "  Processing Page 65 (raw length: 3622 chars, cleaned length: 3355 chars)\n",
            "    Generated 2 chunks for page 65.\n",
            "  Processing Page 66 (raw length: 3571 chars, cleaned length: 3304 chars)\n",
            "    Generated 2 chunks for page 66.\n",
            "  Processing Page 67 (raw length: 3864 chars, cleaned length: 3596 chars)\n",
            "    Generated 2 chunks for page 67.\n",
            "  Processing Page 68 (raw length: 3565 chars, cleaned length: 3309 chars)\n",
            "    Generated 2 chunks for page 68.\n",
            "  Processing Page 69 (raw length: 3304 chars, cleaned length: 3048 chars)\n",
            "    Generated 2 chunks for page 69.\n",
            "  Processing Page 70 (raw length: 3646 chars, cleaned length: 3357 chars)\n",
            "    Generated 2 chunks for page 70.\n",
            "  Processing Page 71 (raw length: 3483 chars, cleaned length: 3214 chars)\n",
            "    Generated 2 chunks for page 71.\n",
            "  Processing Page 72 (raw length: 3895 chars, cleaned length: 3637 chars)\n",
            "    Generated 2 chunks for page 72.\n",
            "  Processing Page 73 (raw length: 3125 chars, cleaned length: 2853 chars)\n",
            "    Generated 2 chunks for page 73.\n",
            "  Processing Page 74 (raw length: 3731 chars, cleaned length: 3472 chars)\n",
            "    Generated 2 chunks for page 74.\n",
            "  Processing Page 75 (raw length: 3888 chars, cleaned length: 3622 chars)\n",
            "    Generated 2 chunks for page 75.\n",
            "  Processing Page 76 (raw length: 3469 chars, cleaned length: 3210 chars)\n",
            "    Generated 2 chunks for page 76.\n",
            "  Processing Page 77 (raw length: 3338 chars, cleaned length: 3073 chars)\n",
            "    Generated 2 chunks for page 77.\n",
            "  Processing Page 78 (raw length: 3064 chars, cleaned length: 2798 chars)\n",
            "    Generated 2 chunks for page 78.\n",
            "  Processing Page 79 (raw length: 3659 chars, cleaned length: 3397 chars)\n",
            "    Generated 2 chunks for page 79.\n",
            "  Processing Page 80 (raw length: 3108 chars, cleaned length: 2846 chars)\n",
            "    Generated 2 chunks for page 80.\n",
            "  Processing Page 81 (raw length: 3382 chars, cleaned length: 3107 chars)\n",
            "    Generated 2 chunks for page 81.\n",
            "  Processing Page 82 (raw length: 3507 chars, cleaned length: 3244 chars)\n",
            "    Generated 2 chunks for page 82.\n",
            "  Processing Page 83 (raw length: 3907 chars, cleaned length: 3654 chars)\n",
            "    Generated 2 chunks for page 83.\n",
            "  Processing Page 84 (raw length: 3323 chars, cleaned length: 3054 chars)\n",
            "    Generated 2 chunks for page 84.\n",
            "  Processing Page 85 (raw length: 3453 chars, cleaned length: 3184 chars)\n",
            "    Generated 2 chunks for page 85.\n",
            "  Processing Page 86 (raw length: 3845 chars, cleaned length: 3577 chars)\n",
            "    Generated 2 chunks for page 86.\n",
            "  Processing Page 87 (raw length: 3565 chars, cleaned length: 3306 chars)\n",
            "    Generated 2 chunks for page 87.\n",
            "  Processing Page 88 (raw length: 3557 chars, cleaned length: 3300 chars)\n",
            "    Generated 2 chunks for page 88.\n",
            "  Processing Page 89 (raw length: 3024 chars, cleaned length: 2769 chars)\n",
            "    Generated 2 chunks for page 89.\n",
            "  Processing Page 90 (raw length: 2957 chars, cleaned length: 2693 chars)\n",
            "    Generated 2 chunks for page 90.\n",
            "  Processing Page 91 (raw length: 4003 chars, cleaned length: 3720 chars)\n",
            "    Generated 2 chunks for page 91.\n",
            "  Processing Page 92 (raw length: 3537 chars, cleaned length: 3264 chars)\n",
            "    Generated 2 chunks for page 92.\n",
            "  Processing Page 93 (raw length: 4060 chars, cleaned length: 3394 chars)\n",
            "    Generated 2 chunks for page 93.\n",
            "  Processing Page 94 (raw length: 3430 chars, cleaned length: 2757 chars)\n",
            "    Generated 2 chunks for page 94.\n",
            "  Processing Page 95 (raw length: 3767 chars, cleaned length: 3180 chars)\n",
            "    Generated 2 chunks for page 95.\n",
            "  Processing Page 96 (raw length: 3489 chars, cleaned length: 2927 chars)\n",
            "    Generated 2 chunks for page 96.\n",
            "  Processing Page 97 (raw length: 3366 chars, cleaned length: 2940 chars)\n",
            "    Generated 2 chunks for page 97.\n",
            "  Processing Page 98 (raw length: 1792 chars, cleaned length: 1502 chars)\n",
            "    Generated 1 chunks for page 98.\n",
            "  Processing Page 99 (raw length: 1655 chars, cleaned length: 1367 chars)\n",
            "    Generated 1 chunks for page 99.\n",
            "  Processing Page 100 (raw length: 1943 chars, cleaned length: 1655 chars)\n",
            "    Generated 1 chunks for page 100.\n",
            "  Processing Page 101 (raw length: 1719 chars, cleaned length: 1476 chars)\n",
            "    Generated 1 chunks for page 101.\n",
            "\n",
            "--- Chunking Document: EDLHLGA23009V012223 ---\n",
            "  Processing Page 1 (raw length: 3763 chars, cleaned length: 3763 chars)\n",
            "    Generated 3 chunks for page 1.\n",
            "  Processing Page 2 (raw length: 2964 chars, cleaned length: 2907 chars)\n",
            "    Generated 2 chunks for page 2.\n",
            "\n",
            "--- Chunking Document: HDFHLIP23024V072223 ---\n",
            "  Processing Page 1 (raw length: 3691 chars, cleaned length: 3691 chars)\n",
            "    Generated 2 chunks for page 1.\n",
            "  Processing Page 2 (raw length: 3431 chars, cleaned length: 3308 chars)\n",
            "    Generated 2 chunks for page 2.\n",
            "  Processing Page 3 (raw length: 3394 chars, cleaned length: 3247 chars)\n",
            "    Generated 2 chunks for page 3.\n",
            "  Processing Page 4 (raw length: 4116 chars, cleaned length: 4001 chars)\n",
            "    Generated 3 chunks for page 4.\n",
            "  Processing Page 5 (raw length: 3826 chars, cleaned length: 3723 chars)\n",
            "    Generated 2 chunks for page 5.\n",
            "  Processing Page 6 (raw length: 3814 chars, cleaned length: 3712 chars)\n",
            "    Generated 2 chunks for page 6.\n",
            "  Processing Page 7 (raw length: 3720 chars, cleaned length: 3614 chars)\n",
            "    Generated 2 chunks for page 7.\n",
            "  Processing Page 8 (raw length: 3638 chars, cleaned length: 3521 chars)\n",
            "    Generated 2 chunks for page 8.\n",
            "  Processing Page 9 (raw length: 3209 chars, cleaned length: 3060 chars)\n",
            "    Generated 2 chunks for page 9.\n",
            "  Processing Page 10 (raw length: 3347 chars, cleaned length: 3208 chars)\n",
            "    Generated 2 chunks for page 10.\n",
            "  Processing Page 11 (raw length: 2872 chars, cleaned length: 2745 chars)\n",
            "    Generated 2 chunks for page 11.\n",
            "  Processing Page 12 (raw length: 2950 chars, cleaned length: 2833 chars)\n",
            "    Generated 2 chunks for page 12.\n",
            "  Processing Page 13 (raw length: 3429 chars, cleaned length: 3315 chars)\n",
            "    Generated 2 chunks for page 13.\n",
            "  Processing Page 14 (raw length: 2613 chars, cleaned length: 2427 chars)\n",
            "    Generated 2 chunks for page 14.\n",
            "  Processing Page 15 (raw length: 2979 chars, cleaned length: 2860 chars)\n",
            "    Generated 2 chunks for page 15.\n",
            "  Processing Page 16 (raw length: 3087 chars, cleaned length: 2956 chars)\n",
            "    Generated 2 chunks for page 16.\n",
            "  Processing Page 17 (raw length: 2633 chars, cleaned length: 2509 chars)\n",
            "    Generated 2 chunks for page 17.\n",
            "  Processing Page 18 (raw length: 3481 chars, cleaned length: 3401 chars)\n",
            "    Generated 2 chunks for page 18.\n",
            "  Processing Page 19 (raw length: 3281 chars, cleaned length: 3182 chars)\n",
            "    Generated 2 chunks for page 19.\n",
            "  Processing Page 20 (raw length: 3466 chars, cleaned length: 3370 chars)\n",
            "    Generated 2 chunks for page 20.\n",
            "  Processing Page 21 (raw length: 3680 chars, cleaned length: 3590 chars)\n",
            "    Generated 2 chunks for page 21.\n",
            "  Processing Page 22 (raw length: 3607 chars, cleaned length: 3485 chars)\n",
            "    Generated 2 chunks for page 22.\n",
            "  Processing Page 23 (raw length: 3475 chars, cleaned length: 3386 chars)\n",
            "    Generated 2 chunks for page 23.\n",
            "  Processing Page 24 (raw length: 3725 chars, cleaned length: 3629 chars)\n",
            "    Generated 2 chunks for page 24.\n",
            "  Processing Page 25 (raw length: 3142 chars, cleaned length: 3043 chars)\n",
            "    Generated 2 chunks for page 25.\n",
            "  Processing Page 26 (raw length: 3430 chars, cleaned length: 3311 chars)\n",
            "    Generated 2 chunks for page 26.\n",
            "  Processing Page 27 (raw length: 3364 chars, cleaned length: 3246 chars)\n",
            "    Generated 2 chunks for page 27.\n",
            "  Processing Page 28 (raw length: 3750 chars, cleaned length: 3646 chars)\n",
            "    Generated 2 chunks for page 28.\n",
            "  Processing Page 29 (raw length: 3305 chars, cleaned length: 3209 chars)\n",
            "    Generated 2 chunks for page 29.\n",
            "  Processing Page 30 (raw length: 2129 chars, cleaned length: 2033 chars)\n",
            "    Generated 1 chunks for page 30.\n",
            "  Processing Page 31 (raw length: 2423 chars, cleaned length: 2334 chars)\n",
            "    Generated 2 chunks for page 31.\n",
            "  Processing Page 32 (raw length: 2715 chars, cleaned length: 2620 chars)\n",
            "    Generated 2 chunks for page 32.\n",
            "  Processing Page 33 (raw length: 2205 chars, cleaned length: 2072 chars)\n",
            "    Generated 2 chunks for page 33.\n",
            "  Processing Page 34 (raw length: 2838 chars, cleaned length: 2670 chars)\n",
            "    Generated 2 chunks for page 34.\n",
            "  Processing Page 35 (raw length: 2514 chars, cleaned length: 2331 chars)\n",
            "    Generated 2 chunks for page 35.\n",
            "  Processing Page 36 (raw length: 2856 chars, cleaned length: 2699 chars)\n",
            "    Generated 2 chunks for page 36.\n",
            "  Processing Page 37 (raw length: 1874 chars, cleaned length: 1725 chars)\n",
            "    Generated 1 chunks for page 37.\n",
            "  Processing Page 38 (raw length: 1566 chars, cleaned length: 1459 chars)\n",
            "    Generated 1 chunks for page 38.\n",
            "  Processing Page 39 (raw length: 1121 chars, cleaned length: 1049 chars)\n",
            "    Generated 1 chunks for page 39.\n",
            "\n",
            "--- Chunking Document: ICIHLIP22012V012223 ---\n",
            "  Processing Page 1 (raw length: 5000 chars, cleaned length: 5000 chars)\n",
            "    Generated 3 chunks for page 1.\n",
            "  Processing Page 2 (raw length: 5378 chars, cleaned length: 5162 chars)\n",
            "    Generated 3 chunks for page 2.\n",
            "  Processing Page 3 (raw length: 5220 chars, cleaned length: 5014 chars)\n",
            "    Generated 3 chunks for page 3.\n",
            "  Processing Page 4 (raw length: 5510 chars, cleaned length: 5328 chars)\n",
            "    Generated 3 chunks for page 4.\n",
            "  Processing Page 5 (raw length: 4938 chars, cleaned length: 4791 chars)\n",
            "    Generated 3 chunks for page 5.\n",
            "  Processing Page 6 (raw length: 4724 chars, cleaned length: 4560 chars)\n",
            "    Generated 3 chunks for page 6.\n",
            "  Processing Page 7 (raw length: 4806 chars, cleaned length: 4651 chars)\n",
            "    Generated 3 chunks for page 7.\n",
            "  Processing Page 8 (raw length: 5213 chars, cleaned length: 5055 chars)\n",
            "    Generated 3 chunks for page 8.\n",
            "  Processing Page 9 (raw length: 4974 chars, cleaned length: 4795 chars)\n",
            "    Generated 3 chunks for page 9.\n",
            "  Processing Page 10 (raw length: 4711 chars, cleaned length: 4559 chars)\n",
            "    Generated 3 chunks for page 10.\n",
            "  Processing Page 11 (raw length: 5416 chars, cleaned length: 5199 chars)\n",
            "    Generated 3 chunks for page 11.\n",
            "  Processing Page 12 (raw length: 4156 chars, cleaned length: 3909 chars)\n",
            "    Generated 3 chunks for page 12.\n",
            "  Processing Page 13 (raw length: 5087 chars, cleaned length: 4826 chars)\n",
            "    Generated 3 chunks for page 13.\n",
            "  Processing Page 14 (raw length: 4618 chars, cleaned length: 4417 chars)\n",
            "    Generated 3 chunks for page 14.\n",
            "  Processing Page 15 (raw length: 5544 chars, cleaned length: 5398 chars)\n",
            "    Generated 3 chunks for page 15.\n",
            "  Processing Page 16 (raw length: 5136 chars, cleaned length: 4934 chars)\n",
            "    Generated 3 chunks for page 16.\n",
            "  Processing Page 17 (raw length: 4375 chars, cleaned length: 4135 chars)\n",
            "    Generated 3 chunks for page 17.\n",
            "  Processing Page 18 (raw length: 1965 chars, cleaned length: 1942 chars)\n",
            "    Generated 1 chunks for page 18.\n",
            "  Processing Page 19 (raw length: 3872 chars, cleaned length: 3638 chars)\n",
            "    Generated 2 chunks for page 19.\n",
            "  Processing Page 20 (raw length: 5235 chars, cleaned length: 5040 chars)\n",
            "    Generated 3 chunks for page 20.\n",
            "  Processing Page 21 (raw length: 4892 chars, cleaned length: 4656 chars)\n",
            "    Generated 3 chunks for page 21.\n",
            "  Processing Page 22 (raw length: 5233 chars, cleaned length: 4885 chars)\n",
            "    Generated 3 chunks for page 22.\n",
            "  Processing Page 23 (raw length: 4933 chars, cleaned length: 4672 chars)\n",
            "    Generated 3 chunks for page 23.\n",
            "  Processing Page 24 (raw length: 5018 chars, cleaned length: 4803 chars)\n",
            "    Generated 3 chunks for page 24.\n",
            "  Processing Page 25 (raw length: 4956 chars, cleaned length: 4776 chars)\n",
            "    Generated 3 chunks for page 25.\n",
            "  Processing Page 26 (raw length: 3257 chars, cleaned length: 2964 chars)\n",
            "    Generated 2 chunks for page 26.\n",
            "  Processing Page 27 (raw length: 3368 chars, cleaned length: 3265 chars)\n",
            "    Generated 2 chunks for page 27.\n",
            "  Processing Page 28 (raw length: 4081 chars, cleaned length: 3949 chars)\n",
            "    Generated 3 chunks for page 28.\n",
            "  Processing Page 29 (raw length: 4864 chars, cleaned length: 4703 chars)\n",
            "    Generated 3 chunks for page 29.\n",
            "  Processing Page 30 (raw length: 4632 chars, cleaned length: 4520 chars)\n",
            "    Generated 3 chunks for page 30.\n",
            "  Processing Page 31 (raw length: 1917 chars, cleaned length: 1878 chars)\n",
            "    Generated 1 chunks for page 31.\n",
            "\n",
            "--- Total Generated Chunks: 505 ---\n",
            "\n",
            "--- Sample of Processed Chunks (first 5) ---\n",
            "\n",
            "Chunk 1:\n",
            "  Metadata: {\n",
            "  \"doc_id\": \"BAJHLIP23020V012223\",\n",
            "  \"page\": 1,\n",
            "  \"clause_id\": \"Clause-BAJHLIP23020V012223-p1-c1\",\n",
            "  \"chunk_length_tokens\": 471,\n",
            "  \"chunk_length_chars\": 1986\n",
            "}\n",
            "  Content (first 200 chars): \"UIN- BAJHLIP23020V012223 \n",
            "                                Global Health Care/ Policy Wordings/Page 1 \n",
            " \n",
            " \n",
            "Bajaj Allianz General Insurance Co. Ltd.                       \n",
            "Bajaj Allianz House, Airport R...\"\n",
            "  Content length (chars): 1986\n",
            "  Content length (tokens): 471\n",
            "\n",
            "Chunk 2:\n",
            "  Metadata: {\n",
            "  \"doc_id\": \"BAJHLIP23020V012223\",\n",
            "  \"page\": 1,\n",
            "  \"clause_id\": \"Clause-BAJHLIP23020V012223-p1-c2\",\n",
            "  \"chunk_length_tokens\": 410,\n",
            "  \"chunk_length_chars\": 1963\n",
            "}\n",
            "  Content (first 200 chars): \"references to the singular or to the masculine, include references to the plural or to the feminine wherever the context \n",
            "permits. If any word starts with Capital alphabet but is not defined in the St...\"\n",
            "  Content length (chars): 1963\n",
            "  Content length (tokens): 410\n",
            "\n",
            "Chunk 3:\n",
            "  Metadata: {\n",
            "  \"doc_id\": \"BAJHLIP23020V012223\",\n",
            "  \"page\": 1,\n",
            "  \"clause_id\": \"Clause-BAJHLIP23020V012223-p1-c3\",\n",
            "  \"chunk_length_tokens\": 220,\n",
            "  \"chunk_length_chars\": 1076\n",
            "}\n",
            "  Content (first 200 chars): \"surgical procedures are to be carried out; \n",
            "iv.Maintaining daily records of the patients and making them accessible to the Insurance Company’s \n",
            "authorized representative. \n",
            " \n",
            "4. \n",
            "AYUSH Day Care Centre:...\"\n",
            "  Content length (chars): 1076\n",
            "  Content length (tokens): 220\n",
            "\n",
            "Chunk 4:\n",
            "  Metadata: {\n",
            "  \"doc_id\": \"BAJHLIP23020V012223\",\n",
            "  \"page\": 2,\n",
            "  \"clause_id\": \"Clause-BAJHLIP23020V012223-p2-c1\",\n",
            "  \"chunk_length_tokens\": 490,\n",
            "  \"chunk_length_chars\": 1999\n",
            "}\n",
            "  Content (first 200 chars): \"UIN- BAJHLIP23020V012223\n",
            "Global Health Care/ Policy Wordings/Page 2\n",
            "Bajaj Allianz General Insurance Co. Ltd.\n",
            "Bajaj Allianz House, Airport Road, Yerawada, Pune - 411 006. Reg. No.: 113\n",
            "For more details...\"\n",
            "  Content length (chars): 1999\n",
            "  Content length (tokens): 490\n",
            "\n",
            "Chunk 5:\n",
            "  Metadata: {\n",
            "  \"doc_id\": \"BAJHLIP23020V012223\",\n",
            "  \"page\": 2,\n",
            "  \"clause_id\": \"Clause-BAJHLIP23020V012223-p2-c2\",\n",
            "  \"chunk_length_tokens\": 405,\n",
            "  \"chunk_length_chars\": 2016\n",
            "}\n",
            "  Content (first 200 chars): \"medical set -up with a Hospital and which has been registered with the local authorities, wherever applicable, and\n",
            "is under supervision of a registered and qualified Medical Practitioner AND must comp...\"\n",
            "  Content length (chars): 2016\n",
            "  Content length (tokens): 405\n",
            "\n",
            "--- Document Processing & Chunking Complete ---\n"
          ]
        }
      ]
    }
  ]
}